{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Evaluation\n",
    "\n",
    "## 1 - Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set this to the path where the data is \n",
    "DATA_ROOT = 'C:\\openmic-2018\\openmic-2018'\n",
    "\n",
    "if not os.path.exists(DATA_ROOT):\n",
    "    raise ValueError('Did you forget to set `DATA_ROOT`?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "OPENMIC = np.load(os.path.join(DATA_ROOT, 'openmic-2018.npz'), allow_pickle=True)\n",
    "\n",
    "# Make direct variable names for everything\n",
    "X, Y_true, Y_mask, sample_key = OPENMIC['X'], OPENMIC['Y_true'], OPENMIC['Y_mask'], OPENMIC['sample_key']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map class indices to names\n",
    "with open(os.path.join(DATA_ROOT, 'class-map.json'), 'r') as f:\n",
    "    class_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load the splits\n",
    "### Creating splits for train, test, and the unlabeled data.\n",
    "###### Adapted from the original OpenMIC notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, test, and unlabeled sets\n",
    "# Use squeeze=True here to return a single array for each, rather than a full DataFrame\n",
    "\n",
    "split_train = pd.read_csv(os.path.join(DATA_ROOT, 'partitions/split01_train.csv'), \n",
    "                          header=None).squeeze(\"columns\")\n",
    "split_test = pd.read_csv(os.path.join(DATA_ROOT, 'partitions/split01_test.csv'), \n",
    "                         header=None).squeeze(\"columns\")\n",
    "\n",
    "# Create partition CSV for unlabeled\n",
    "split_unlabeled = pd.read_csv(os.path.join(DATA_ROOT, 'partitions/split01_unlabeled.csv'), \n",
    "                         header=None).squeeze(\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The breakdown of the data is roughly 50% training, 25% test, 25% unlabeled\n",
    "# The percentage breakdowns can be adjusted by adjusting the partitions csv's above \n",
    "print('# Train: {},  # Test: {}, # Unlabeled: {}'.format(len(split_train), len(split_test), len(split_unlabeled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = set(split_train)\n",
    "test_set = set(split_test)\n",
    "unlabeled_set = set(split_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into arrays\n",
    "\n",
    "idx_train, idx_test, idx_unlabeled = [], [], []\n",
    "\n",
    "for idx, n in enumerate(sample_key):\n",
    "    if n in train_set:\n",
    "        idx_train.append(idx)\n",
    "    elif n in test_set:\n",
    "        idx_test.append(idx)\n",
    "    elif n in unlabeled_set:\n",
    "        idx_unlabeled.append(idx)\n",
    "    else:\n",
    "        raise RuntimeError('Unknown sample key={}! Abort!'.format(sample_key[n]))\n",
    "\n",
    "# Cast the idx_* arrays to numpy structures\n",
    "idx_train = np.asarray(idx_train)\n",
    "idx_test = np.asarray(idx_test)\n",
    "idx_unlabeled = np.asarray(idx_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we use the split indices to partition the features, labels, and masks\n",
    "X_train = X[idx_train]\n",
    "X_test = X[idx_test]\n",
    "X_unlabeled = X[idx_unlabeled]\n",
    "\n",
    "Y_true_train = Y_true[idx_train]\n",
    "Y_true_test = Y_true[idx_test]\n",
    "Y_true_unlabeled = Y_true[idx_unlabeled]\n",
    "\n",
    "Y_mask_train = Y_mask[idx_train]\n",
    "Y_mask_test = Y_mask[idx_test]\n",
    "Y_mask_unlabeled = Y_mask[idx_unlabeled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate shapes of slices\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_unlabeled.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Fit the models\n",
    "### The below has been updated from the original OpenMIC notebook to include both Random Forest and KNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = dict()\n",
    "\n",
    "for instrument in class_map:\n",
    "\n",
    "    # get column num from instrument name\n",
    "    inst_num = class_map[instrument]\n",
    "\n",
    "    # isolate data that has been labeled as this instrument\n",
    "    train_inst = Y_mask_train[:, inst_num]\n",
    "    test_inst = Y_mask_test[:, inst_num]\n",
    "\n",
    "    # gets training data with labels for this instrument\n",
    "    X_train_inst = X_train[train_inst]\n",
    "\n",
    "    # averages features over time\n",
    "    X_train_inst_sklearn = np.mean(X_train_inst, axis=1)\n",
    "\n",
    "    # labels instrument as present if value over 0.5\n",
    "    Y_true_train_inst = Y_true_train[train_inst, inst_num] >= 0.5\n",
    "\n",
    "    # Repeat slicing for test\n",
    "    X_test_inst = X_test[test_inst]\n",
    "    X_test_inst_sklearn = np.mean(X_test_inst, axis=1)\n",
    "    Y_true_test_inst = Y_true_test[test_inst, inst_num] >= 0.5\n",
    "\n",
    "    # Initialize a new classifier\n",
    "    rfc = RandomForestClassifier(max_depth=8, n_estimators=100, random_state=0)\n",
    "    knn = KNeighborsClassifier(n_neighbors=10)   #TODO: BUMP UP # OF NEIGHBORS\n",
    "\n",
    "    # Fit model\n",
    "    rfc.fit(X_train_inst_sklearn, Y_true_train_inst)\n",
    "    knn.fit(X_train_inst_sklearn, Y_true_train_inst)\n",
    "\n",
    "    # Evaluate the model\n",
    "    Y_pred_train_rfc = rfc.predict(X_train_inst_sklearn)\n",
    "    Y_pred_test_rfc = rfc.predict(X_test_inst_sklearn)\n",
    "\n",
    "    Y_pred_train_knn = knn.predict(X_train_inst_sklearn)\n",
    "    Y_pred_test_knn = knn.predict(X_test_inst_sklearn)\n",
    "\n",
    "    print('-' * 52)\n",
    "    print(instrument)\n",
    "    print('\\tTRAIN RFC')\n",
    "    print(classification_report(Y_true_train_inst, Y_pred_train_rfc))\n",
    "    print('\\tTEST RFC')\n",
    "    print(classification_report(Y_true_test_inst, Y_pred_test_rfc))\n",
    "    print('\\tTRAIN knn')\n",
    "    print(classification_report(Y_true_train_inst, Y_pred_train_knn))\n",
    "    print('\\tTEST knn')\n",
    "    print(classification_report(Y_true_test_inst, Y_pred_test_knn))\n",
    "    \n",
    "    # Store the classifier in our dictionary\n",
    "    models[instrument] = [rfc, knn]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmic Disagreement\n",
    "\n",
    "#### In the algorithimc disagreement process, two models evaluate the same piece of data and their evaluations are compared. If they disagree on their evaluation, that track is deemed to be a priority for annotation.\n",
    "##### Let's start with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need soundfile to load audio data\n",
    "import soundfile as sf\n",
    "import sys\n",
    "\n",
    "# For audio playback\n",
    "from IPython.display import Audio\n",
    "\n",
    "from compare import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run algorithmic disagreement process\n",
    "\n",
    "# Populate skipIndices with empty arrays to be filled\n",
    "skipIndices = {}\n",
    "for i in class_map:\n",
    "    skipIndices[i] = []\n",
    "\n",
    "uncertaintyScores, allPredictions = compare(X_unlabeled, models, skipIndices=skipIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(uncertaintyScores[\"voice\"].items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ID of the most uncertain track for voice\n",
    "highest_idx = list(uncertaintyScores.get(\"voice\").items())[0][0]\n",
    "trackID = split_unlabeled[highest_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the most uncertain track is...\n",
    "trackID"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hear what this difficult-to-label track sounds like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace below .ogg file with trackID\n",
    "audio, rate = sf.read(os.path.join(DATA_ROOT, 'audio/000/000493_42240.ogg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's listen to the example\n",
    "Audio(data=audio.T, rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this displays the instruments that the models disagreed on\n",
    "allPredictions.get(\"voice\").get(highest_idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental Evaluation -- Simulate the Annotation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, train a model on the fully annotated dataset. This is what we will be measuring efficiency against. \n",
    "combined_models = dict()\n",
    "\n",
    "for instrument in class_map:\n",
    "\n",
    "    # get column num from instrument name\n",
    "    inst_num = class_map[instrument]\n",
    "\n",
    "    # isolate data that has been labeled as this instrument\n",
    "    train_inst = Y_mask_train[:, inst_num]\n",
    "    test_inst = Y_mask_test[:, inst_num]\n",
    "    unlabeled_inst  = Y_mask_unlabeled[:, inst_num]\n",
    "\n",
    "    # gets training data with labels for this instrument\n",
    "    X_train_inst = X_train[train_inst]\n",
    "    X_unlabeled_inst = X_unlabeled[unlabeled_inst]\n",
    "\n",
    "    # combine training and unlabeled sets\n",
    "    X_combined = np.append(X_train_inst, X_unlabeled_inst, axis=0)\n",
    "    Y_true_combined = np.append(Y_true_train, Y_true_unlabeled, axis=0)\n",
    "\n",
    "    # averages features over time\n",
    "    X_train_inst_sklearn = np.mean(X_combined, axis=1)\n",
    "\n",
    "    # labels instrument as present if value over 0.5\n",
    "    Y_true_train_inst = Y_true_train[train_inst, inst_num] >= 0.5\n",
    "    Y_true_unlabeled_inst = Y_true_unlabeled[unlabeled_inst, inst_num] >= 0.5\n",
    "\n",
    "    Y_true_combined = np.append(Y_true_train_inst, Y_true_unlabeled_inst, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    # Repeat slicing for test\n",
    "    X_test_inst = X_test[test_inst]\n",
    "    X_test_inst_sklearn = np.mean(X_test_inst, axis=1)\n",
    "    Y_true_test_inst = Y_true_test[test_inst, inst_num] >= 0.5\n",
    "\n",
    "    # Initialize a new classifier\n",
    "    rfc = RandomForestClassifier(max_depth=8, n_estimators=100, random_state=0)\n",
    "\n",
    "    # Fit model\n",
    "    rfc.fit(X_train_inst_sklearn, Y_true_combined)\n",
    "\n",
    "    # Evaluate the model\n",
    "    Y_pred_train_rfc = rfc.predict(X_train_inst_sklearn)\n",
    "    Y_pred_test_rfc = rfc.predict(X_test_inst_sklearn)\n",
    "\n",
    "    combined_models[instrument] = rfc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TO_LABEL = 50    # Batch size\n",
    "NUM_RANDOM = 5      # Number of random tracks to include \n",
    "EPOCHS = 10          # Number of times to annotate\n",
    "\n",
    "X_copy = {}\n",
    "skipIndices = {}\n",
    "accuracies_over_time = {}\n",
    "ctrl_accuracies = {}\n",
    "\n",
    "for i in class_map:\n",
    "    skipIndices[i] = []\n",
    "    accuracies_over_time[i] = []\n",
    "    ctrl_accuracies[i] = []\n",
    "\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    # get instrument predictions\n",
    "    sorted_trx, y = compare(X_unlabeled, models, skipIndices=skipIndices)\n",
    "\n",
    "    trained_models = {}\n",
    "\n",
    "\n",
    "    for instr in sorted_trx:    \n",
    "        X_labeled = []     # Track IDs for labeled tracks will be added here\n",
    "        Y_true_labeled = []      # Track IDs for labeled tracks will be added here\n",
    "        Y_mask_labeled = []      # Track IDs for labeled tracks will be added here\n",
    "        track_indices = list(sorted_trx[instr].items())[:5]\n",
    "        track_indices = [i[0] for i in track_indices]   # isolates the indices\n",
    "\n",
    "        # add random tracks to be annotated\n",
    "        track_indices = addRandomTracks(NUM_RANDOM, len(sorted_trx[instr]), track_indices)\n",
    "\n",
    "        # add track IDs to the labeled list\n",
    "        for trk in track_indices:\n",
    "            X_labeled.append(X_unlabeled[trk])\n",
    "            Y_true_labeled.append(Y_true_unlabeled[trk])\n",
    "            Y_mask_labeled.append(Y_mask_unlabeled[trk])\n",
    "\n",
    "        # Update indices to skip here\n",
    "        skipIndices[instr] = np.append(skipIndices[instr], track_indices)\n",
    "\n",
    "        # Now to train a new model on the annotated data\n",
    "        \n",
    "        ###########################################################################\n",
    "        inst_num = class_map[instr]\n",
    "        model = trainModel(inst_num, X_train, X_test, X_labeled, Y_true_train, Y_true_test, np.array(Y_true_labeled), Y_mask_train, Y_mask_test, np.array(Y_mask_labeled))\n",
    "\n",
    "        trained_models[instr] = model\n",
    "\n",
    "        # Graph the accuracy results\n",
    "        control_model = combined_models[instr]\n",
    "\n",
    "        test_inst = Y_mask_test[:, inst_num]\n",
    "\n",
    "        # Repeat slicing for test\n",
    "        X_test_inst = X_test[test_inst]\n",
    "        X_test_inst_sklearn = np.mean(X_test_inst, axis=1)\n",
    "        Y_true_test_inst = Y_true_test[test_inst, inst_num] >= 0.5\n",
    "\n",
    "        Y_pred_test_ctrl = control_model.predict(X_test_inst_sklearn)\n",
    "        Y_pred_test_rfc = model.predict(X_test_inst_sklearn)\n",
    "\n",
    "        # Graph the accuracy results\n",
    "        control_model = combined_models[instr]\n",
    "\n",
    "        test_inst = Y_mask_test[:, inst_num]\n",
    "\n",
    "        # Repeat slicing for test\n",
    "        X_test_inst = X_test[test_inst]\n",
    "        X_test_inst_sklearn = np.mean(X_test_inst, axis=1)\n",
    "        Y_true_test_inst = Y_true_test[test_inst, inst_num] >= 0.5\n",
    "\n",
    "        Y_pred_test_ctrl = control_model.predict(X_test_inst_sklearn)\n",
    "        Y_pred_test_rfc = model.predict(X_test_inst_sklearn)\n",
    "\n",
    "        acc = classification_report(Y_true_test_inst, Y_pred_test_rfc, output_dict=True)[\"accuracy\"]  \n",
    "        ctrl_acc = classification_report(Y_true_test_inst, Y_pred_test_ctrl, output_dict=True)[\"accuracy\"]  \n",
    "\n",
    "        accuracies_over_time[instr].append(acc)\n",
    "        ctrl_accuracies[instr].append(ctrl_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in class_map:\n",
    "    plt.plot(accuracies_over_time[i], linestyle='dotted')\n",
    "    plt.plot(ctrl_accuracies[i])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update\n",
    "* Annotation process is simulated\n",
    "\n",
    "To Do\n",
    "* Evaluation - train model on partially-annotated dataset\n",
    "* Update README"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
