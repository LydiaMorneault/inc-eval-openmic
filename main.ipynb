{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Evaluation\n",
    "\n",
    "## 1 - Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from compare import *\n",
    "\n",
    "# Set this to the path where the data is \n",
    "DATA_ROOT = 'C:\\openmic-2018\\openmic-2018'\n",
    "\n",
    "if not os.path.exists(DATA_ROOT):\n",
    "    raise ValueError('Did you forget to set `DATA_ROOT`?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "OPENMIC = np.load(os.path.join(DATA_ROOT, 'openmic-2018.npz'), allow_pickle=True)\n",
    "\n",
    "# Make direct variable names for everything\n",
    "X, Y_true, Y_mask, sample_key = OPENMIC['X'], OPENMIC['Y_true'], OPENMIC['Y_mask'], OPENMIC['sample_key']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map class indices to names\n",
    "with open(os.path.join(DATA_ROOT, 'class-map.json'), 'r') as f:\n",
    "    class_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accordion': 0,\n",
       " 'banjo': 1,\n",
       " 'bass': 2,\n",
       " 'cello': 3,\n",
       " 'clarinet': 4,\n",
       " 'cymbals': 5,\n",
       " 'drums': 6,\n",
       " 'flute': 7,\n",
       " 'guitar': 8,\n",
       " 'mallet_percussion': 9,\n",
       " 'mandolin': 10,\n",
       " 'organ': 11,\n",
       " 'piano': 12,\n",
       " 'saxophone': 13,\n",
       " 'synthesizer': 14,\n",
       " 'trombone': 15,\n",
       " 'trumpet': 16,\n",
       " 'ukulele': 17,\n",
       " 'violin': 18,\n",
       " 'voice': 19}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load the splits\n",
    "### Creating splits for train, test, and the unlabeled data.\n",
    "###### Adapted from the original OpenMIC notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, test, and unlabeled sets\n",
    "# Use squeeze=True here to return a single array for each, rather than a full DataFrame\n",
    "\n",
    "split_train = pd.read_csv(os.path.join(DATA_ROOT, 'partitions/split01_train.csv'), \n",
    "                          header=None).squeeze(\"columns\")\n",
    "split_test = pd.read_csv(os.path.join(DATA_ROOT, 'partitions/split01_test.csv'), \n",
    "                         header=None).squeeze(\"columns\")\n",
    "\n",
    "# Create partition CSV for unlabeled\n",
    "split_unlabeled = pd.read_csv(os.path.join(DATA_ROOT, 'partitions/split01_unlabeled.csv'), \n",
    "                         header=None).squeeze(\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train: 9993,  # Test: 5085, # Unlabeled: 4922\n"
     ]
    }
   ],
   "source": [
    "# The breakdown of the data is roughly 50% training, 25% test, 25% unlabeled\n",
    "# The percentage breakdowns can be adjusted by adjusting the partitions csv's above \n",
    "print('# Train: {},  # Test: {}, # Unlabeled: {}'.format(len(split_train), len(split_test), len(split_unlabeled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = set(split_train)\n",
    "test_set = set(split_test)\n",
    "unlabeled_set = set(split_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into arrays\n",
    "\n",
    "idx_train, idx_test, idx_unlabeled = [], [], []\n",
    "\n",
    "for idx, n in enumerate(sample_key):\n",
    "    if n in train_set:\n",
    "        idx_train.append(idx)\n",
    "    elif n in test_set:\n",
    "        idx_test.append(idx)\n",
    "    elif n in unlabeled_set:\n",
    "        idx_unlabeled.append(idx)\n",
    "    else:\n",
    "        raise RuntimeError('Unknown sample key={}! Abort!'.format(sample_key[n]))\n",
    "\n",
    "# Cast the idx_* arrays to numpy structures\n",
    "idx_train = np.asarray(idx_train)\n",
    "idx_test = np.asarray(idx_test)\n",
    "idx_unlabeled = np.asarray(idx_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's included in the data?\n",
    "\n",
    "- `X`: 20000 * 10 * 128 array of VGGish features\n",
    "    - First index (0..19999) corresponds to the sample key\n",
    "    - Second index (0..9) corresponds to the time within the clip (each time slice is 960 ms long)\n",
    "    - Third index (0..127) corresponds to the VGGish features at each point in the 10sec clip\n",
    "    - Example `X[40, 8]` is the 128-dimensional feature vector for the 9th time slice in the 41st example\n",
    "- `Y_true`: 20000 * 20 array of *true* label probabilities\n",
    "    - First index corresponds to sample key, as above\n",
    "    - Second index corresponds to the label class (accordion, ..., voice)\n",
    "    - Example: `Y[40, 4]` indicates the confidence that example #41 contains the 5th instrument\n",
    "- `Y_mask`: 20000 * 20 binary mask values\n",
    "    - First index corresponds to sample key\n",
    "    - Second index corresponds to the label class\n",
    "    - Example: `Y[40, 4]` indicates whether or not we have observations for the 5th instrument for example #41\n",
    "- `sample_key`: 20000 array of sample key strings\n",
    "    - Example: `sample_key[40]` is the sample key for example #41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we use the split indices to partition the features, labels, and masks\n",
    "X_train = X[idx_train]\n",
    "X_test = X[idx_test]\n",
    "X_unlabeled = X[idx_unlabeled]\n",
    "\n",
    "Y_true_train = Y_true[idx_train]\n",
    "Y_true_test = Y_true[idx_test]\n",
    "Y_true_unlabeled = Y_true[idx_unlabeled]\n",
    "\n",
    "Y_mask_train = Y_mask[idx_train]\n",
    "Y_mask_test = Y_mask[idx_test]\n",
    "Y_mask_unlabeled = Y_mask[idx_unlabeled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9993, 10, 128)\n",
      "(5085, 10, 128)\n",
      "(4922, 10, 128)\n"
     ]
    }
   ],
   "source": [
    "# Validate shapes of slices\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_unlabeled.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Fit the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = dict()\n",
    "\n",
    "for instrument in class_map:\n",
    "\n",
    "    # Map the instrument name to its column number\n",
    "    inst_num = class_map[instrument]\n",
    "\n",
    "    rfc = trainModel(\"rfc\",inst_num, X_train, Y_true_train, Y_mask_train)[0]\n",
    "    knn = trainModel(\"knn\",inst_num, X_train, Y_true_train, Y_mask_train)[0]\n",
    "   \n",
    "    # Store the classifier in our dictionary\n",
    "    models[instrument] = [rfc, knn]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Algorithmic Disagreement\n",
    "\n",
    "#### In the algorithimc disagreement process, two models evaluate the same piece of data and their evaluations are compared. If they disagree on their evaluation, that track is deemed to be a priority for annotation.\n",
    "##### Let's start with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need soundfile to load audio data\n",
    "import soundfile as sf\n",
    "\n",
    "# For audio playback\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run algorithmic disagreement process\n",
    "\n",
    "# Populate skipIndices with empty arrays to be filled\n",
    "skipIndices = {}\n",
    "for i in class_map:\n",
    "    skipIndices[i] = []\n",
    "\n",
    "\n",
    "# uncertaintyScores, allPredictions = compare(X_unlabeled, models, skipIndices=skipIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncertaintyScores[\"voice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(uncertaintyScores[\"voice\"].items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ID of the most uncertain track for voice\n",
    "# highest_idx = list(uncertaintyScores.get(\"voice\").items())[0][0]\n",
    "# trackID = split_unlabeled[highest_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the most uncertain track is...\n",
    "# trackID"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hear what this difficult-to-label track sounds like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace below .ogg file with trackID\n",
    "# audio, rate = sf.read(os.path.join(DATA_ROOT, 'audio/027/027639_257280.ogg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's listen to the example\n",
    "# Audio(data=audio.T, rate=rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Incremental Evaluation -- Simulate the Annotation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, train a model on the fully annotated dataset. This is what we will be measuring efficiency against. \n",
    "combined_models = dict()\n",
    "ctrl_true_vals = dict()\n",
    "\n",
    "for instrument in class_map:\n",
    "\n",
    "    # get column num from instrument name\n",
    "    inst_num = class_map[instrument]\n",
    "\n",
    "    rfc, props = trainModel(\"rfc\",inst_num, X_train, Y_true_train, Y_mask_train)\n",
    "    combined_models[instrument] = rfc\n",
    "\n",
    "    # Repeat slicing for test\n",
    "    test_inst = Y_mask_test[:, inst_num]\n",
    "    X_test_inst = X_test[test_inst]\n",
    "    X_test_inst_sklearn = np.mean(X_test_inst, axis=1)\n",
    "    Y_true_test_inst = Y_true_test[test_inst, inst_num] >= 0.5\n",
    "\n",
    "\n",
    "    ctrl_true_vals[instrument] = Y_true_test_inst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accordion starting\n",
      "banjo starting\n",
      "bass starting\n",
      "cello starting\n",
      "clarinet starting\n",
      "cymbals starting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\18607\\Github\\inc-eval-openmic\\main.ipynb Cell 28\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/18607/Github/inc-eval-openmic/main.ipynb#X36sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# MAIN LOOP\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/18607/Github/inc-eval-openmic/main.ipynb#X36sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/18607/Github/inc-eval-openmic/main.ipynb#X36sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/18607/Github/inc-eval-openmic/main.ipynb#X36sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m# get instrument predictions\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/18607/Github/inc-eval-openmic/main.ipynb#X36sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     sorted_trx, allInstProbs \u001b[39m=\u001b[39m compare(X_unlabeled, models, skipIndices\u001b[39m=\u001b[39mskipIndices)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/18607/Github/inc-eval-openmic/main.ipynb#X36sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mfor\u001b[39;00m instr \u001b[39min\u001b[39;00m sorted_trx:    \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/18607/Github/inc-eval-openmic/main.ipynb#X36sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/18607/Github/inc-eval-openmic/main.ipynb#X36sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m         \u001b[39m###########################################################################\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/18607/Github/inc-eval-openmic/main.ipynb#X36sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         \u001b[39m# SELECT TRACKS FOR ANNOTATION\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/18607/Github/inc-eval-openmic/main.ipynb#X36sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/18607/Github/inc-eval-openmic/main.ipynb#X36sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39m# Retrieve the top tracks for annotation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/18607/Github/inc-eval-openmic/main.ipynb#X36sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         track_indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(sorted_trx[instr]\u001b[39m.\u001b[39mitems())[:NUM_TO_LABEL]\n",
      "File \u001b[1;32mc:\\Users\\18607\\Github\\inc-eval-openmic\\compare.py:53\u001b[0m, in \u001b[0;36mcompare\u001b[1;34m(X, models, skipIndices)\u001b[0m\n\u001b[0;32m     50\u001b[0m feature_mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(X[trk], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     52\u001b[0m \u001b[39m# Each model makes a prediction\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m rfcPred \u001b[39m=\u001b[39m rfc\u001b[39m.\u001b[39;49mpredict_proba(feature_mean)[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m]\n\u001b[0;32m     54\u001b[0m knnPred \u001b[39m=\u001b[39m knn\u001b[39m.\u001b[39mpredict_proba(feature_mean)[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m]\n\u001b[0;32m     56\u001b[0m instrPreds[trk] \u001b[39m=\u001b[39m [rfcPred, knnPred]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:885\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    880\u001b[0m all_proba \u001b[39m=\u001b[39m [\n\u001b[0;32m    881\u001b[0m     np\u001b[39m.\u001b[39mzeros((X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], j), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[0;32m    882\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39matleast_1d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_)\n\u001b[0;32m    883\u001b[0m ]\n\u001b[0;32m    884\u001b[0m lock \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mLock()\n\u001b[1;32m--> 885\u001b[0m Parallel(n_jobs\u001b[39m=\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, require\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msharedmem\u001b[39;49m\u001b[39m\"\u001b[39;49m)(\n\u001b[0;32m    886\u001b[0m     delayed(_accumulate_prediction)(e\u001b[39m.\u001b[39;49mpredict_proba, X, all_proba, lock)\n\u001b[0;32m    887\u001b[0m     \u001b[39mfor\u001b[39;49;00m e \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimators_\n\u001b[0;32m    888\u001b[0m )\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m proba \u001b[39min\u001b[39;00m all_proba:\n\u001b[0;32m    891\u001b[0m     proba \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:664\u001b[0m, in \u001b[0;36m_accumulate_prediction\u001b[1;34m(predict, X, out, lock)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_accumulate_prediction\u001b[39m(predict, X, out, lock):\n\u001b[0;32m    658\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[39m    This is a utility function for joblib's Parallel.\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \n\u001b[0;32m    661\u001b[0m \u001b[39m    It can't go locally in ForestClassifier or ForestRegressor, because joblib\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[39m    complains that it cannot pickle it when placed there.\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     prediction \u001b[39m=\u001b[39m predict(X, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    665\u001b[0m     \u001b[39mwith\u001b[39;00m lock:\n\u001b[0;32m    666\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:1003\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.predict_proba\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m   1001\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m   1002\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_X_predict(X, check_input)\n\u001b[1;32m-> 1003\u001b[0m proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[0;32m   1005\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1006\u001b[0m     proba \u001b[39m=\u001b[39m proba[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "NUM_TO_LABEL = 500  # Batch size\n",
    "EPOCHS = 5       # Number of times to annotate\n",
    "\n",
    "# NUM_RANDOM = 50      # Number of random tracks to include \n",
    "\n",
    "# Set Up\n",
    "skipIndices = {}\n",
    "accuracies_over_time = {}\n",
    "ctrl_accuracies = {}\n",
    "instrProps = {}\n",
    "reports = {}\n",
    "f1s = {}\n",
    "# Populate dictionaries with instrument keys\n",
    "for i in class_map:\n",
    "    skipIndices[i] = []\n",
    "    accuracies_over_time[i] = []\n",
    "    ctrl_accuracies[i] = []\n",
    "    instrProps[i] = []\n",
    "    reports[i] = []\n",
    "    f1s[i] = []\n",
    "\n",
    "X_labeled = []           # Track IDs for labeled tracks\n",
    "Y_true_labeled = []      # True values for labeled tracks\n",
    "\n",
    "# MAIN LOOP\n",
    "for i in range(EPOCHS):\n",
    "\n",
    "    # get instrument predictions\n",
    "    sorted_trx, allInstProbs = compare(X_unlabeled, models, skipIndices=skipIndices)\n",
    "\n",
    "    for instr in sorted_trx:    \n",
    "\n",
    "        ###########################################################################\n",
    "        # SELECT TRACKS FOR ANNOTATION\n",
    "\n",
    "        # Retrieve the top tracks for annotation\n",
    "        track_indices = list(sorted_trx[instr].items())[:NUM_TO_LABEL]\n",
    "        track_indices = [i[0] for i in track_indices]   # isolates the indices\n",
    "\n",
    "        # add random tracks to be annotated to avoid false positives\n",
    "        # track_indices = addRandomTracks(NUM_RANDOM, len(sorted_trx[instr]), track_indices, allInstProbs[instr])\n",
    "\n",
    "        # add track IDs to the labeled list\n",
    "        for trk in track_indices:\n",
    "            X_labeled.append(X_unlabeled[trk])\n",
    "\n",
    "            # If the track has not yet been labeled for this instrument, update with avg prediction\n",
    "            if Y_true_unlabeled[trk][class_map[instr]] == 0.5:\n",
    "                Y_true_unlabeled[trk][class_map[instr]] = np.average(allInstProbs[instr][trk])\n",
    "        \n",
    "            Y_true_labeled.append(Y_true_unlabeled[trk])\n",
    "            Y_mask_unlabeled[trk][class_map[instr]] = True\n",
    "\n",
    "        # Update indices to skip here\n",
    "        skipIndices[instr] = np.append(skipIndices[instr], track_indices)\n",
    "\n",
    "        ###########################################################################\n",
    "        # TRAIN MODEL ON PARTIALLY ANNOTATED DATASET\n",
    "        \n",
    "        inst_num = class_map[instr]\n",
    "        model, props = trainModel(modelType=\"rfc\",inst_num=inst_num, X_train=X_train, \\\n",
    "                X_labeled=X_labeled, Y_true_train=Y_true_train, Y_true_labeled=np.array(Y_true_labeled), \\\n",
    "                Y_mask_train=Y_mask_train)\n",
    "        \n",
    "        instrProps[instr].append(props)\n",
    "\n",
    "        # Repeat slicing for test\n",
    "        test_inst = Y_mask_test[:, inst_num]\n",
    "        X_test_inst = X_test[test_inst]\n",
    "        X_test_inst_sklearn = np.mean(X_test_inst, axis=1)\n",
    "        Y_true_test_inst = Y_true_test[test_inst, inst_num] >= 0.5\n",
    "\n",
    "        ###########################################################################\n",
    "        # COMPARE ACCURACY WITH FULLY ANNOTATED MODEL\n",
    "\n",
    "        # Graph the accuracy results\n",
    "        control_model = combined_models[instr]\n",
    "\n",
    "        Y_pred_test_ctrl = control_model.predict(X_test_inst_sklearn)\n",
    "        Y_pred_test_rfc = model.predict(X_test_inst_sklearn)\n",
    "\n",
    "        rprt = classification_report(Y_true_test_inst, Y_pred_test_rfc, output_dict=True) \n",
    "        ctrl_rprt = classification_report(ctrl_true_vals[instr], Y_pred_test_ctrl, output_dict=True)\n",
    "\n",
    "        acc = rprt[\"accuracy\"]\n",
    "        ctrl_acc = ctrl_rprt[\"accuracy\"]\n",
    "\n",
    "\n",
    "        f1 = rprt['weighted avg']['f1-score']\n",
    "\n",
    "        accuracies_over_time[instr].append(acc)\n",
    "        ctrl_accuracies[instr].append(ctrl_acc)\n",
    "        f1s[instr].append(f1)\n",
    "        reports[instr].append(rprt)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in class_map:\n",
    "\n",
    "#     # plt.figure(figsize=(6,6))\n",
    "\n",
    "#     plt.bar([\"train positive\", \"train negative\"], height=instrProps[i][j].get('train'))\n",
    "#     plt.bar([\"test positive\", \"test negative\"], height=instrProps[i][j].get('test'))\n",
    "\n",
    "#     plt.xlabel(\"Epochs\")\n",
    "#     plt.ylabel(\"Number of Tracks\")\n",
    "#     plt.title(i)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.arange(len(class_map))  # the label locations\n",
    "# width = 0.25  # the width of the bars\n",
    "# multiplier = 0\n",
    "\n",
    "# fig, ax = plt.subplots(layout='constrained', figsize=(12,4))\n",
    "\n",
    "# for train in instrProps[i][0].items():\n",
    "#     val = train\n",
    "#     offset = width * multiplier\n",
    "#     rects = ax.bar(x + offset, train[1][0], width, label=train[0])\n",
    "#     ax.bar_label(rects, padding=3)\n",
    "#     multiplier += 1\n",
    "\n",
    "# # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "# ax.set_ylabel('Number of Tracks')\n",
    "# ax.set_title('Positive vs. Negative Examples')\n",
    "# ax.set_xticks(x + width, class_map)\n",
    "# ax.legend(loc='upper left', ncols=3)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in class_map:\n",
    "    plt.plot(accuracies_over_time[i], linestyle='dotted')\n",
    "    plt.plot(ctrl_accuracies[i])\n",
    "    plt.plot(f1s[i], linestyle='dashed')\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.xticks(np.arange(len(accuracies_over_time[i])), np.arange(1, len(accuracies_over_time[i])+1))\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average accuracy across instruments\n",
    "partial = []\n",
    "control = []\n",
    "\n",
    "for n in range(EPOCHS):\n",
    "    t = [accuracies_over_time[i][n] for i in accuracies_over_time]   # isolates the indices\n",
    "    b = [ctrl_accuracies[i][n] for i in ctrl_accuracies]   # isolates the indices\n",
    "    partial.append(np.average(t))\n",
    "    control.append(np.average(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(partial, linestyle='dotted')\n",
    "plt.plot(control)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(np.arange(len(accuracies_over_time[i])), np.arange(1, len(accuracies_over_time[i])+1))\n",
    "plt.ylabel(\"Average Accuracy\")\n",
    "plt.title(\"Accuracy Across all Instruments\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02260bda173dc4cfac9721b963d852cbd460b4b9f259c47413be5d43f6c0ea6b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
